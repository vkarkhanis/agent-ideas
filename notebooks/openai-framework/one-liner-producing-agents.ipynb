{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-26T06:26:08.379849Z",
     "start_time": "2026-01-26T06:26:08.372265Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.tool import FunctionTool\n",
    "import os, requests, json\n",
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:03:25.564956Z",
     "start_time": "2026-01-26T06:03:25.550069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv(override=True)\n",
    "perplexity_api_key = os.getenv('PERPLEXITY_API_KEY')\n",
    "\n",
    "if perplexity_api_key:\n",
    "    print(f\"Perplexity API Key exists and begins {perplexity_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Perplexity API Key not set (and this is optional)\")\n",
    "\n",
    "model_name=\"gpt-5-nano\"\n",
    "perp_model_name=\"sonar\""
   ],
   "id": "a99d557553420b8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity API Key exists and begins pplx\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:03:26.992721Z",
     "start_time": "2026-01-26T06:03:26.985931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perplexity tool\n",
    "def search_tool_impl(query: str) -> str:\n",
    "    print(f\"Searching {query}\")\n",
    "    response = requests.post(\n",
    "        url=\"https://api.perplexity.ai/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {perplexity_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": perp_model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json().choices[0].message.content\n",
    "\n",
    "search_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"query\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"query\"]\n",
    "}\n",
    "\n",
    "def on_invoke_search(ctx, args):\n",
    "    return search_tool_impl(args[\"query\"])\n",
    "\n",
    "search_tool = FunctionTool(\n",
    "    name=\"search_tool\",\n",
    "    description=\"Search the web using Perplexity\",\n",
    "    params_json_schema=search_schema,\n",
    "    on_invoke_tool=on_invoke_search\n",
    ")\n",
    "\n"
   ],
   "id": "7965430eb9e2dedb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:26:11.305124Z",
     "start_time": "2026-01-26T06:26:11.293817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Printing tool\n",
    "def print_tool_impl(text_msg: str):\n",
    "    print(text_msg)\n",
    "\n",
    "print_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"text_msg\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"text_msg\"]\n",
    "}\n",
    "\n",
    "async def on_invoke_print(ctx, args):\n",
    "    if isinstance(args, str):\n",
    "        try:\n",
    "            args = json.loads(args)   # case: JSON string\n",
    "        except json.JSONDecodeError:\n",
    "            text = args              # case: scalar string\n",
    "        else:\n",
    "            text = args[\"text_msg\"]  # decoded object\n",
    "    else:\n",
    "        text = args[\"text_msg\"]      # already dict\n",
    "\n",
    "    print_tool_impl(text)\n",
    "    return \"OK\"\n",
    "\n",
    "print_tool = FunctionTool(\n",
    "    name=\"print_tool\",\n",
    "    description=\"Print the text_msg\",\n",
    "    params_json_schema=print_schema,\n",
    "    on_invoke_tool=on_invoke_print\n",
    ")"
   ],
   "id": "49d8d4100c5bc44f",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:29:43.224918Z",
     "start_time": "2026-01-26T06:29:43.217949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instructions1 = \"You are an advertising executive and a rather funny but intelligent and creative one. You are supposed to provide one liner to sale a product given by user. Provide only  the one liner and nothing else. You live upto your expectations of creating a vaey creative and smart one liner for advertising the given product. You will be provided by the user of the product, country if use and purpose of use which you can utilise to make the selection\"\n",
    "\n",
    "instructions2 = \"You are an advertising executive, intelligent but very professional and serious. You are supposed to provide one liner to sale a product given by user. Provide only  the one liner and nothing else. You live upto your expectations of creating a serious one liner for advertising the given product. You will be provided by the user of the product, country if use and purpose of use which you can utilise to make the selection\"\n",
    "\n",
    "instructions3 = \"You are an advertising executive. You are given one liners and are supposed to choose the best one liner for the given product. Just choose the best one liner and nothing else\"\n",
    "\n",
    "common_message = \"Use search_tool for generating results. Use new one-liners not the existing ones\"\n"
   ],
   "id": "1cfe9993fbc842ac",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:29:46.525904Z",
     "start_time": "2026-01-26T06:29:46.518925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "advertising_agent_1 = Agent(\n",
    "    name=\"Advertising agent 1\",\n",
    "    instructions=instructions1 + common_message,\n",
    "    tools=[search_tool]\n",
    ")\n",
    "\n",
    "advertising_agent_2 = Agent(\n",
    "    name=\"Advertising agent 2\",\n",
    "    instructions=instructions2 + common_message,\n",
    "    tools=[search_tool]\n",
    ")"
   ],
   "id": "c73ed4a75ab58213",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:27:23.115082Z",
     "start_time": "2026-01-26T06:27:20.839027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = Runner.run_streamed(advertising_agent_2, input=\"Kookabura cricket bat. Please mention the name of the LLM model used\")\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ],
   "id": "ada1f02b68f26d0d",
   "outputs": [
    {
     "ename": "UserError",
     "evalue": "Error running tool search_tool: string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\_run_impl.py:976\u001B[39m, in \u001B[36mRunImpl.execute_function_tool_calls.<locals>.run_single_tool\u001B[39m\u001B[34m(func_tool, tool_call)\u001B[39m\n\u001B[32m    974\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    975\u001B[39m     \u001B[38;5;66;03m# 2) Actually run the tool\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m976\u001B[39m     real_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._execute_tool_with_hooks(\n\u001B[32m    977\u001B[39m         func_tool=func_tool,\n\u001B[32m    978\u001B[39m         tool_context=tool_context,\n\u001B[32m    979\u001B[39m         agent=agent,\n\u001B[32m    980\u001B[39m         hooks=hooks,\n\u001B[32m    981\u001B[39m         tool_call=tool_call,\n\u001B[32m    982\u001B[39m     )\n\u001B[32m    984\u001B[39m     \u001B[38;5;66;03m# 3) Run output tool guardrails, if any\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\_run_impl.py:933\u001B[39m, in \u001B[36mRunImpl._execute_tool_with_hooks\u001B[39m\u001B[34m(cls, func_tool, tool_context, agent, hooks, tool_call)\u001B[39m\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m    925\u001B[39m     hooks.on_tool_start(tool_context, agent, func_tool),\n\u001B[32m    926\u001B[39m     (\n\u001B[32m   (...)\u001B[39m\u001B[32m    930\u001B[39m     ),\n\u001B[32m    931\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m933\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[43mfunc_tool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mon_invoke_tool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtool_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_call\u001B[49m\u001B[43m.\u001B[49m\u001B[43marguments\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 28\u001B[39m, in \u001B[36mon_invoke_search\u001B[39m\u001B[34m(ctx, args)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mon_invoke_search\u001B[39m(ctx, args):\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m search_tool_impl(\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquery\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[31mTypeError\u001B[39m: string indices must be integers, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mUserError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[107]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m result = Runner.run_streamed(advertising_agent_2, \u001B[38;5;28minput\u001B[39m=\u001B[33m\"\u001B[39m\u001B[33mKookabura cricket bat. Please mention the name of the LLM model used\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m result.stream_events():\n\u001B[32m      3\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m event.type == \u001B[33m\"\u001B[39m\u001B[33mraw_response_event\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event.data, ResponseTextDeltaEvent):\n\u001B[32m      4\u001B[39m         \u001B[38;5;28mprint\u001B[39m(event.data.delta, end=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m, flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\result.py:358\u001B[39m, in \u001B[36mRunResultStreaming.stream_events\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    355\u001B[39m         \u001B[38;5;28mself\u001B[39m._cleanup_tasks()\n\u001B[32m    357\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stored_exception:\n\u001B[32m--> \u001B[39m\u001B[32m358\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stored_exception\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:1230\u001B[39m, in \u001B[36mAgentRunner._start_streaming\u001B[39m\u001B[34m(cls, starting_input, streamed_result, starting_agent, max_turns, hooks, context_wrapper, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001B[39m\n\u001B[32m   1219\u001B[39m     streamed_result._input_guardrails_task = asyncio.create_task(\n\u001B[32m   1220\u001B[39m         \u001B[38;5;28mcls\u001B[39m._run_input_guardrails_with_queue(\n\u001B[32m   1221\u001B[39m             starting_agent,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1227\u001B[39m         )\n\u001B[32m   1228\u001B[39m     )\n\u001B[32m   1229\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1230\u001B[39m     turn_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._run_single_turn_streamed(\n\u001B[32m   1231\u001B[39m         streamed_result,\n\u001B[32m   1232\u001B[39m         current_agent,\n\u001B[32m   1233\u001B[39m         hooks,\n\u001B[32m   1234\u001B[39m         context_wrapper,\n\u001B[32m   1235\u001B[39m         run_config,\n\u001B[32m   1236\u001B[39m         should_run_agent_start_hooks,\n\u001B[32m   1237\u001B[39m         tool_use_tracker,\n\u001B[32m   1238\u001B[39m         all_tools,\n\u001B[32m   1239\u001B[39m         server_conversation_tracker,\n\u001B[32m   1240\u001B[39m     )\n\u001B[32m   1241\u001B[39m     should_run_agent_start_hooks = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   1243\u001B[39m     streamed_result.raw_responses = streamed_result.raw_responses + [\n\u001B[32m   1244\u001B[39m         turn_result.model_response\n\u001B[32m   1245\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:1580\u001B[39m, in \u001B[36mAgentRunner._run_single_turn_streamed\u001B[39m\u001B[34m(cls, streamed_result, agent, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, all_tools, server_conversation_tracker)\u001B[39m\n\u001B[32m   1577\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ModelBehaviorError(\u001B[33m\"\u001B[39m\u001B[33mModel did not produce a final response!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1579\u001B[39m \u001B[38;5;66;03m# 3. Now, we can process the turn as we do in the non-streaming case\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1580\u001B[39m single_step_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._get_single_step_result_from_response(\n\u001B[32m   1581\u001B[39m     agent=agent,\n\u001B[32m   1582\u001B[39m     original_input=streamed_result.input,\n\u001B[32m   1583\u001B[39m     pre_step_items=streamed_result._model_input_items,\n\u001B[32m   1584\u001B[39m     new_response=final_response,\n\u001B[32m   1585\u001B[39m     output_schema=output_schema,\n\u001B[32m   1586\u001B[39m     all_tools=all_tools,\n\u001B[32m   1587\u001B[39m     handoffs=handoffs,\n\u001B[32m   1588\u001B[39m     hooks=hooks,\n\u001B[32m   1589\u001B[39m     context_wrapper=context_wrapper,\n\u001B[32m   1590\u001B[39m     run_config=run_config,\n\u001B[32m   1591\u001B[39m     tool_use_tracker=tool_use_tracker,\n\u001B[32m   1592\u001B[39m     event_queue=streamed_result._event_queue,\n\u001B[32m   1593\u001B[39m )\n\u001B[32m   1595\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdataclasses\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_dc\u001B[39;00m\n\u001B[32m   1597\u001B[39m \u001B[38;5;66;03m# Stream session items (unfiltered) when available for observability.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:1753\u001B[39m, in \u001B[36mAgentRunner._get_single_step_result_from_response\u001B[39m\u001B[34m(cls, agent, all_tools, original_input, pre_step_items, new_response, output_schema, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, event_queue)\u001B[39m\n\u001B[32m   1750\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m handoff_items:\n\u001B[32m   1751\u001B[39m         RunImpl.stream_step_items_to_queue(cast(\u001B[38;5;28mlist\u001B[39m[RunItem], handoff_items), event_queue)\n\u001B[32m-> \u001B[39m\u001B[32m1753\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m RunImpl.execute_tools_and_side_effects(\n\u001B[32m   1754\u001B[39m     agent=agent,\n\u001B[32m   1755\u001B[39m     original_input=original_input,\n\u001B[32m   1756\u001B[39m     pre_step_items=pre_step_items,\n\u001B[32m   1757\u001B[39m     new_response=new_response,\n\u001B[32m   1758\u001B[39m     processed_response=processed_response,\n\u001B[32m   1759\u001B[39m     output_schema=output_schema,\n\u001B[32m   1760\u001B[39m     hooks=hooks,\n\u001B[32m   1761\u001B[39m     context_wrapper=context_wrapper,\n\u001B[32m   1762\u001B[39m     run_config=run_config,\n\u001B[32m   1763\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\_run_impl.py:316\u001B[39m, in \u001B[36mRunImpl.execute_tools_and_side_effects\u001B[39m\u001B[34m(cls, agent, original_input, pre_step_items, new_response, processed_response, output_schema, hooks, context_wrapper, run_config)\u001B[39m\n\u001B[32m    306\u001B[39m new_step_items.extend(processed_response.new_items)\n\u001B[32m    308\u001B[39m \u001B[38;5;66;03m# First, run function tools, computer actions, shell calls, apply_patch calls,\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[38;5;66;03m# and legacy local shell calls.\u001B[39;00m\n\u001B[32m    310\u001B[39m (\n\u001B[32m    311\u001B[39m     (function_results, tool_input_guardrail_results, tool_output_guardrail_results),\n\u001B[32m    312\u001B[39m     computer_results,\n\u001B[32m    313\u001B[39m     shell_results,\n\u001B[32m    314\u001B[39m     apply_patch_results,\n\u001B[32m    315\u001B[39m     local_shell_results,\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m ) = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m    317\u001B[39m     \u001B[38;5;28mcls\u001B[39m.execute_function_tool_calls(\n\u001B[32m    318\u001B[39m         agent=agent,\n\u001B[32m    319\u001B[39m         tool_runs=processed_response.functions,\n\u001B[32m    320\u001B[39m         hooks=hooks,\n\u001B[32m    321\u001B[39m         context_wrapper=context_wrapper,\n\u001B[32m    322\u001B[39m         config=run_config,\n\u001B[32m    323\u001B[39m     ),\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mcls\u001B[39m.execute_computer_actions(\n\u001B[32m    325\u001B[39m         agent=agent,\n\u001B[32m    326\u001B[39m         actions=processed_response.computer_actions,\n\u001B[32m    327\u001B[39m         hooks=hooks,\n\u001B[32m    328\u001B[39m         context_wrapper=context_wrapper,\n\u001B[32m    329\u001B[39m         config=run_config,\n\u001B[32m    330\u001B[39m     ),\n\u001B[32m    331\u001B[39m     \u001B[38;5;28mcls\u001B[39m.execute_shell_calls(\n\u001B[32m    332\u001B[39m         agent=agent,\n\u001B[32m    333\u001B[39m         calls=processed_response.shell_calls,\n\u001B[32m    334\u001B[39m         hooks=hooks,\n\u001B[32m    335\u001B[39m         context_wrapper=context_wrapper,\n\u001B[32m    336\u001B[39m         config=run_config,\n\u001B[32m    337\u001B[39m     ),\n\u001B[32m    338\u001B[39m     \u001B[38;5;28mcls\u001B[39m.execute_apply_patch_calls(\n\u001B[32m    339\u001B[39m         agent=agent,\n\u001B[32m    340\u001B[39m         calls=processed_response.apply_patch_calls,\n\u001B[32m    341\u001B[39m         hooks=hooks,\n\u001B[32m    342\u001B[39m         context_wrapper=context_wrapper,\n\u001B[32m    343\u001B[39m         config=run_config,\n\u001B[32m    344\u001B[39m     ),\n\u001B[32m    345\u001B[39m     \u001B[38;5;28mcls\u001B[39m.execute_local_shell_calls(\n\u001B[32m    346\u001B[39m         agent=agent,\n\u001B[32m    347\u001B[39m         calls=processed_response.local_shell_calls,\n\u001B[32m    348\u001B[39m         hooks=hooks,\n\u001B[32m    349\u001B[39m         context_wrapper=context_wrapper,\n\u001B[32m    350\u001B[39m         config=run_config,\n\u001B[32m    351\u001B[39m     ),\n\u001B[32m    352\u001B[39m )\n\u001B[32m    353\u001B[39m new_step_items.extend([result.run_item \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m function_results])\n\u001B[32m    354\u001B[39m new_step_items.extend(computer_results)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\_run_impl.py:1025\u001B[39m, in \u001B[36mRunImpl.execute_function_tool_calls\u001B[39m\u001B[34m(cls, agent, tool_runs, hooks, context_wrapper, config)\u001B[39m\n\u001B[32m   1022\u001B[39m     function_tool = tool_run.function_tool\n\u001B[32m   1023\u001B[39m     tasks.append(run_single_tool(function_tool, tool_run.tool_call))\n\u001B[32m-> \u001B[39m\u001B[32m1025\u001B[39m results = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(*tasks)\n\u001B[32m   1027\u001B[39m function_tool_results = [\n\u001B[32m   1028\u001B[39m     FunctionToolResult(\n\u001B[32m   1029\u001B[39m         tool=tool_run.function_tool,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1037\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m tool_run, result \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(tool_runs, results)\n\u001B[32m   1038\u001B[39m ]\n\u001B[32m   1040\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m function_tool_results, tool_input_guardrail_results, tool_output_guardrail_results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\_run_impl.py:1014\u001B[39m, in \u001B[36mRunImpl.execute_function_tool_calls.<locals>.run_single_tool\u001B[39m\u001B[34m(func_tool, tool_call)\u001B[39m\n\u001B[32m   1012\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, AgentsException):\n\u001B[32m   1013\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m UserError(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError running tool \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc_tool.name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m config.trace_include_sensitive_data:\n\u001B[32m   1017\u001B[39m     span_fn.span_data.output = result\n",
      "\u001B[31mUserError\u001B[39m: Error running tool search_tool: string indices must be integers, not 'str'"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "415e0713455c0f26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T05:56:47.481167Z",
     "start_time": "2026-01-26T05:56:46.825311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "product_search_message = \"Jordan shoes\"\n",
    "with trace(\"Parallel one liners\"):\n",
    "    results =await asyncio.gather(\n",
    "        Runner.run(advertising_agent_1, product_search_message),\n",
    "        Runner.run(advertising_agent_2, product_search_message)\n",
    "    )\n",
    "\n",
    "outputs = [result.final_output for result in results]\n",
    "for output in outputs:\n",
    "    print(output + \"\\n\\n\")"
   ],
   "id": "a9f8db100d8ac520",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 400 - {'error': {'message': \"The requested model 'sonar' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}. (request_id: req_54cade09fe374837b6a29b266ebfd11c)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"The requested model 'sonar' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[99]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m product_search_message = \u001B[33m\"\u001B[39m\u001B[33mJordan shoes\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m trace(\u001B[33m\"\u001B[39m\u001B[33mParallel one liners\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     results =\u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m      4\u001B[39m         Runner.run(advertising_agent_1, product_search_message),\n\u001B[32m      5\u001B[39m         Runner.run(advertising_agent_2, product_search_message)\n\u001B[32m      6\u001B[39m     )\n\u001B[32m      8\u001B[39m outputs = [result.final_output \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m results]\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:372\u001B[39m, in \u001B[36mRunner.run\u001B[39m\u001B[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    324\u001B[39m \u001B[33;03mRun a workflow starting at the given agent.\u001B[39;00m\n\u001B[32m    325\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    368\u001B[39m \u001B[33;03m    type of the output.\u001B[39;00m\n\u001B[32m    369\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    371\u001B[39m runner = DEFAULT_AGENT_RUNNER\n\u001B[32m--> \u001B[39m\u001B[32m372\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m runner.run(\n\u001B[32m    373\u001B[39m     starting_agent,\n\u001B[32m    374\u001B[39m     \u001B[38;5;28minput\u001B[39m,\n\u001B[32m    375\u001B[39m     context=context,\n\u001B[32m    376\u001B[39m     max_turns=max_turns,\n\u001B[32m    377\u001B[39m     hooks=hooks,\n\u001B[32m    378\u001B[39m     run_config=run_config,\n\u001B[32m    379\u001B[39m     previous_response_id=previous_response_id,\n\u001B[32m    380\u001B[39m     auto_previous_response_id=auto_previous_response_id,\n\u001B[32m    381\u001B[39m     conversation_id=conversation_id,\n\u001B[32m    382\u001B[39m     session=session,\n\u001B[32m    383\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:671\u001B[39m, in \u001B[36mAgentRunner.run\u001B[39m\u001B[34m(self, starting_agent, input, **kwargs)\u001B[39m\n\u001B[32m    663\u001B[39m     sequential_results = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._run_input_guardrails(\n\u001B[32m    664\u001B[39m         starting_agent,\n\u001B[32m    665\u001B[39m         sequential_guardrails,\n\u001B[32m    666\u001B[39m         _copy_str_or_list(prepared_input),\n\u001B[32m    667\u001B[39m         context_wrapper,\n\u001B[32m    668\u001B[39m     )\n\u001B[32m    670\u001B[39m \u001B[38;5;66;03m# Run parallel guardrails + agent together.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m671\u001B[39m input_guardrail_results, turn_result = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m    672\u001B[39m     \u001B[38;5;28mself\u001B[39m._run_input_guardrails(\n\u001B[32m    673\u001B[39m         starting_agent,\n\u001B[32m    674\u001B[39m         parallel_guardrails,\n\u001B[32m    675\u001B[39m         _copy_str_or_list(prepared_input),\n\u001B[32m    676\u001B[39m         context_wrapper,\n\u001B[32m    677\u001B[39m     ),\n\u001B[32m    678\u001B[39m     \u001B[38;5;28mself\u001B[39m._run_single_turn(\n\u001B[32m    679\u001B[39m         agent=current_agent,\n\u001B[32m    680\u001B[39m         all_tools=all_tools,\n\u001B[32m    681\u001B[39m         original_input=original_input,\n\u001B[32m    682\u001B[39m         generated_items=generated_items,\n\u001B[32m    683\u001B[39m         hooks=hooks,\n\u001B[32m    684\u001B[39m         context_wrapper=context_wrapper,\n\u001B[32m    685\u001B[39m         run_config=run_config,\n\u001B[32m    686\u001B[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001B[32m    687\u001B[39m         tool_use_tracker=tool_use_tracker,\n\u001B[32m    688\u001B[39m         server_conversation_tracker=server_conversation_tracker,\n\u001B[32m    689\u001B[39m     ),\n\u001B[32m    690\u001B[39m )\n\u001B[32m    692\u001B[39m \u001B[38;5;66;03m# Combine sequential and parallel results.\u001B[39;00m\n\u001B[32m    693\u001B[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:1689\u001B[39m, in \u001B[36mAgentRunner._run_single_turn\u001B[39m\u001B[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001B[39m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;28minput\u001B[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001B[32m   1687\u001B[39m     \u001B[38;5;28minput\u001B[39m.extend([generated_item.to_input_item() \u001B[38;5;28;01mfor\u001B[39;00m generated_item \u001B[38;5;129;01min\u001B[39;00m generated_items])\n\u001B[32m-> \u001B[39m\u001B[32m1689\u001B[39m new_response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._get_new_response(\n\u001B[32m   1690\u001B[39m     agent,\n\u001B[32m   1691\u001B[39m     system_prompt,\n\u001B[32m   1692\u001B[39m     \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   1693\u001B[39m     output_schema,\n\u001B[32m   1694\u001B[39m     all_tools,\n\u001B[32m   1695\u001B[39m     handoffs,\n\u001B[32m   1696\u001B[39m     hooks,\n\u001B[32m   1697\u001B[39m     context_wrapper,\n\u001B[32m   1698\u001B[39m     run_config,\n\u001B[32m   1699\u001B[39m     tool_use_tracker,\n\u001B[32m   1700\u001B[39m     server_conversation_tracker,\n\u001B[32m   1701\u001B[39m     prompt_config,\n\u001B[32m   1702\u001B[39m )\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._get_single_step_result_from_response(\n\u001B[32m   1705\u001B[39m     agent=agent,\n\u001B[32m   1706\u001B[39m     original_input=original_input,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1715\u001B[39m     tool_use_tracker=tool_use_tracker,\n\u001B[32m   1716\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\run.py:1952\u001B[39m, in \u001B[36mAgentRunner._get_new_response\u001B[39m\u001B[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001B[39m\n\u001B[32m   1942\u001B[39m previous_response_id = (\n\u001B[32m   1943\u001B[39m     server_conversation_tracker.previous_response_id\n\u001B[32m   1944\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m server_conversation_tracker\n\u001B[32m   1945\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m server_conversation_tracker.previous_response_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1946\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1947\u001B[39m )\n\u001B[32m   1948\u001B[39m conversation_id = (\n\u001B[32m   1949\u001B[39m     server_conversation_tracker.conversation_id \u001B[38;5;28;01mif\u001B[39;00m server_conversation_tracker \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1950\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1952\u001B[39m new_response = \u001B[38;5;28;01mawait\u001B[39;00m model.get_response(\n\u001B[32m   1953\u001B[39m     system_instructions=filtered.instructions,\n\u001B[32m   1954\u001B[39m     \u001B[38;5;28minput\u001B[39m=filtered.input,\n\u001B[32m   1955\u001B[39m     model_settings=model_settings,\n\u001B[32m   1956\u001B[39m     tools=all_tools,\n\u001B[32m   1957\u001B[39m     output_schema=output_schema,\n\u001B[32m   1958\u001B[39m     handoffs=handoffs,\n\u001B[32m   1959\u001B[39m     tracing=get_model_tracing_impl(\n\u001B[32m   1960\u001B[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001B[32m   1961\u001B[39m     ),\n\u001B[32m   1962\u001B[39m     previous_response_id=previous_response_id,\n\u001B[32m   1963\u001B[39m     conversation_id=conversation_id,\n\u001B[32m   1964\u001B[39m     prompt=prompt_config,\n\u001B[32m   1965\u001B[39m )\n\u001B[32m   1967\u001B[39m context_wrapper.usage.add(new_response.usage)\n\u001B[32m   1969\u001B[39m \u001B[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:97\u001B[39m, in \u001B[36mOpenAIResponsesModel.get_response\u001B[39m\u001B[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m response_span(disabled=tracing.is_disabled()) \u001B[38;5;28;01mas\u001B[39;00m span_response:\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m         response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fetch_response(\n\u001B[32m     98\u001B[39m             system_instructions,\n\u001B[32m     99\u001B[39m             \u001B[38;5;28minput\u001B[39m,\n\u001B[32m    100\u001B[39m             model_settings,\n\u001B[32m    101\u001B[39m             tools,\n\u001B[32m    102\u001B[39m             output_schema,\n\u001B[32m    103\u001B[39m             handoffs,\n\u001B[32m    104\u001B[39m             previous_response_id=previous_response_id,\n\u001B[32m    105\u001B[39m             conversation_id=conversation_id,\n\u001B[32m    106\u001B[39m             stream=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    107\u001B[39m             prompt=prompt,\n\u001B[32m    108\u001B[39m         )\n\u001B[32m    110\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001B[32m    111\u001B[39m             logger.debug(\u001B[33m\"\u001B[39m\u001B[33mLLM responded\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\agents\\models\\openai_responses.py:320\u001B[39m, in \u001B[36mOpenAIResponsesModel._fetch_response\u001B[39m\u001B[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001B[39m\n\u001B[32m    316\u001B[39m         response_format = {\u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: model_settings.verbosity}\n\u001B[32m    318\u001B[39m stream_param: Literal[\u001B[38;5;28;01mTrue\u001B[39;00m] | Omit = \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m omit\n\u001B[32m--> \u001B[39m\u001B[32m320\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.responses.create(\n\u001B[32m    321\u001B[39m     previous_response_id=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(previous_response_id),\n\u001B[32m    322\u001B[39m     conversation=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(conversation_id),\n\u001B[32m    323\u001B[39m     instructions=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(system_instructions),\n\u001B[32m    324\u001B[39m     model=model_param,\n\u001B[32m    325\u001B[39m     \u001B[38;5;28minput\u001B[39m=list_input,\n\u001B[32m    326\u001B[39m     include=include,\n\u001B[32m    327\u001B[39m     tools=tools_param,\n\u001B[32m    328\u001B[39m     prompt=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(prompt),\n\u001B[32m    329\u001B[39m     temperature=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.temperature),\n\u001B[32m    330\u001B[39m     top_p=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.top_p),\n\u001B[32m    331\u001B[39m     truncation=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.truncation),\n\u001B[32m    332\u001B[39m     max_output_tokens=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.max_tokens),\n\u001B[32m    333\u001B[39m     tool_choice=tool_choice,\n\u001B[32m    334\u001B[39m     parallel_tool_calls=parallel_tool_calls,\n\u001B[32m    335\u001B[39m     stream=cast(Any, stream_param),\n\u001B[32m    336\u001B[39m     extra_headers=\u001B[38;5;28mself\u001B[39m._merge_headers(model_settings),\n\u001B[32m    337\u001B[39m     extra_query=model_settings.extra_query,\n\u001B[32m    338\u001B[39m     extra_body=model_settings.extra_body,\n\u001B[32m    339\u001B[39m     text=response_format,\n\u001B[32m    340\u001B[39m     store=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.store),\n\u001B[32m    341\u001B[39m     prompt_cache_retention=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.prompt_cache_retention),\n\u001B[32m    342\u001B[39m     reasoning=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.reasoning),\n\u001B[32m    343\u001B[39m     metadata=\u001B[38;5;28mself\u001B[39m._non_null_or_omit(model_settings.metadata),\n\u001B[32m    344\u001B[39m     **extra_args,\n\u001B[32m    345\u001B[39m )\n\u001B[32m    346\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(Union[Response, AsyncStream[ResponseStreamEvent]], response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:2476\u001B[39m, in \u001B[36mAsyncResponses.create\u001B[39m\u001B[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   2438\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   2439\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   2440\u001B[39m     *,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2474\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   2475\u001B[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001B[32m-> \u001B[39m\u001B[32m2476\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   2477\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m/responses\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2478\u001B[39m         body=\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[32m   2479\u001B[39m             {\n\u001B[32m   2480\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mbackground\u001B[39m\u001B[33m\"\u001B[39m: background,\n\u001B[32m   2481\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mconversation\u001B[39m\u001B[33m\"\u001B[39m: conversation,\n\u001B[32m   2482\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33minclude\u001B[39m\u001B[33m\"\u001B[39m: include,\n\u001B[32m   2483\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33minput\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   2484\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33minstructions\u001B[39m\u001B[33m\"\u001B[39m: instructions,\n\u001B[32m   2485\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_output_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_output_tokens,\n\u001B[32m   2486\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: max_tool_calls,\n\u001B[32m   2487\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   2488\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   2489\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   2490\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprevious_response_id\u001B[39m\u001B[33m\"\u001B[39m: previous_response_id,\n\u001B[32m   2491\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt\u001B[39m\u001B[33m\"\u001B[39m: prompt,\n\u001B[32m   2492\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   2493\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_retention\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_retention,\n\u001B[32m   2494\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mreasoning\u001B[39m\u001B[33m\"\u001B[39m: reasoning,\n\u001B[32m   2495\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   2496\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   2497\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   2498\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m   2499\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   2500\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   2501\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m: text,\n\u001B[32m   2502\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   2503\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   2504\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   2505\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   2506\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtruncation\u001B[39m\u001B[33m\"\u001B[39m: truncation,\n\u001B[32m   2507\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   2508\u001B[39m             },\n\u001B[32m   2509\u001B[39m             response_create_params.ResponseCreateParamsStreaming\n\u001B[32m   2510\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m   2511\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001B[32m   2512\u001B[39m         ),\n\u001B[32m   2513\u001B[39m         options=make_request_options(\n\u001B[32m   2514\u001B[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m   2515\u001B[39m         ),\n\u001B[32m   2516\u001B[39m         cast_to=Response,\n\u001B[32m   2517\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   2518\u001B[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001B[32m   2519\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794\u001B[39m, in \u001B[36mAsyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1780\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1781\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1782\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1789\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_AsyncStreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1790\u001B[39m ) -> ResponseT | _AsyncStreamT:\n\u001B[32m   1791\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1792\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), **options\n\u001B[32m   1793\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1794\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\workspace\\personal-agent-ideas\\agent-ideas\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1594\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1591\u001B[39m             \u001B[38;5;28;01mawait\u001B[39;00m err.response.aread()\n\u001B[32m   1593\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1594\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1596\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1598\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'error': {'message': \"The requested model 'sonar' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 400 - {'error': {'message': \"The requested model 'sonar' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}. (request_id: req_e65cec4c9ad346819ede2626738b31f7)\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:32:53.880003Z",
     "start_time": "2026-01-26T06:32:53.869469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_picker_agent = Agent(\n",
    "    instructions=instructions3,\n",
    "    model = model_name,\n",
    "    name=\"Best one liner selector\"\n",
    ")"
   ],
   "id": "c396296dbe8c0207",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:21.533924Z",
     "start_time": "2026-01-26T07:11:21.528624Z"
    }
   },
   "cell_type": "code",
   "source": "product = \"Bagheera from Jungle Book to be purchased by Mike Tyson as pet\"\n",
   "id": "6aedbffb24e535d",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:28:29.976567Z",
     "start_time": "2026-01-26T06:28:22.061931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "with trace(\"Select best one liner\"):\n",
    "    results = await asyncio.gather(\n",
    "        Runner.run(advertising_agent_1, product),\n",
    "        Runner.run(advertising_agent_2, product),\n",
    "    )\n",
    "    outputs = [result.final_output for result in results]\n",
    "    print(outputs)\n",
    "\n",
    "    one_liners = f\"Selected one liner:\" + \"\\n\\nOne Liners:\\n\\n\".join(outputs)\n",
    "    best = await Runner.run(result_picker_agent, one_liners)\n",
    "\n",
    "    print(f\"Best one liner:\\n{best.final_output}\")"
   ],
   "id": "4f76946f13fc2b2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Parle G: The biscuit thats been dunked in more Indian chai than all your childhood secrets!', 'Nourishing Indias moments with every biteParle G, the taste of tradition.']\n",
      "Best one liner:\n",
      "Parle G: The biscuit thats been dunked in more Indian chai than all your childhood secrets!\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Now to use tools\n",
    "### Following tools will be created\n",
    "#### 1. Perplexity search tool\n",
    "#### 2. One-liner producing agents\n",
    "#### 3. Final message printing tool"
   ],
   "id": "d71c8a61388aea61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:08:02.300766Z",
     "start_time": "2026-01-26T07:08:02.296162Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a489ae38a01c4957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T06:59:19.484186Z",
     "start_time": "2026-01-26T06:59:19.475931Z"
    }
   },
   "cell_type": "code",
   "source": "print(search_tool)",
   "id": "ae890f981d619149",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FunctionTool(name='search_tool', description='Search the web using Perplexity', params_json_schema={'type': 'object', 'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'additionalProperties': False}, on_invoke_tool=<function on_invoke_search at 0x000001F58E1B13A0>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:25.993675Z",
     "start_time": "2026-01-26T07:11:25.986534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent tools\n",
    "advertising_agent_1_tool = advertising_agent_1.as_tool(tool_name=\"advertising_agent_tool_1\", tool_description=\"The creative one\")\n",
    "\n",
    "advertising_agent_2_tool = advertising_agent_2.as_tool(tool_name=\"advertising_agent_tool_2\", tool_description=\"The serious one\")"
   ],
   "id": "f7ba20cc24c42ef5",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:29.804416Z",
     "start_time": "2026-01-26T07:11:29.798400Z"
    }
   },
   "cell_type": "code",
   "source": "tools = [search_tool, advertising_agent_1_tool, advertising_agent_2_tool, print_tool]",
   "id": "3a7a26747bae22a9",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:31.781980Z",
     "start_time": "2026-01-26T07:11:31.774884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selection_agent_instructions = \"\"\"\n",
    "You are a smart one-liner selector. Your goal is to find the best one-liner in the given context. Please follow the instructions carefully.\n",
    "\n",
    "1.Generate draft one-liners: Use both the advertising agent tools to generate two different one-liner drafts. Do not proceed until both the drafts are ready\n",
    "\n",
    "2. Review: the drafts and use your judgement to select the single best one-liner in an unbiased way. Use the context used by the advertising agent tools to make your judgement.\n",
    "\n",
    "3. Print: You use print_tool to print the selected result. You must ALWAYS also hand off to the manager agent to format and print the result again in formatted form\n",
    "\n",
    "Crucial rules:\n",
    "- Use the advertising tools agents to create draft. Dont create your own\n",
    "- Select only one draft one-liners as best one-liner and nothing else\n",
    "- Use the Model from provided tools\n",
    "\n",
    "\"\"\""
   ],
   "id": "f58e722faf58b400",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Handoff Agent",
   "id": "ca4c1930ee43f16d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:34.518907Z",
     "start_time": "2026-01-26T07:11:34.509385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "format_tool_instruction = \"You are a formatting and translation expert. You convert any English text to Marathi and print it\"\n",
    "\n",
    "format_agent = Agent(name=\"Format agent\", instructions=format_tool_instruction, model=model_name)\n",
    "format_tool =format_agent.as_tool(tool_name=\"format_tool\", tool_description=\"Format the drafts\")\n",
    "\n",
    "handoff_tools = [format_tool]\n",
    "\n",
    "# handoff_manager_instructions = \"You are an agent to format text. Use tool format_tool to format the given text and print formatted text. Print only formatted text as final output\"\n",
    "handoff_manager_instructions = \"You are a formatting and translation expert. You convert any English text to Marathi and print it. Use print tool to print the text\"\n",
    "\n",
    "handoff_manager = Agent(instructions=handoff_manager_instructions,name=\"Handsoff Agent\", handoff_description=\"Handoff agent to be used by one-liner selector\", tools=[print_tool])"
   ],
   "id": "364cd4aa5d8ca346",
   "outputs": [],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:06:32.705387Z",
     "start_time": "2026-01-26T07:06:32.701460Z"
    }
   },
   "cell_type": "code",
   "source": "print(handoff_manager)",
   "id": "37eeaeb0311efe97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent(name='Handsoff Agent', handoff_description='Handoff agent to be used by one-liner selector', tools=[FunctionTool(name='print_tool', description='Print the text_msg', params_json_schema={'type': 'object', 'properties': {'text_msg': {'type': 'string'}}, 'required': ['text_msg'], 'additionalProperties': False}, on_invoke_tool=<function on_invoke_print at 0x000001F5950B8F40>, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)], mcp_servers=[], mcp_config={}, instructions='You are a formatting and translation expert. You convert any English text to Marathi and print it. Use print tool to print the text', prompt=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, prompt_cache_retention=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:37.187214Z",
     "start_time": "2026-01-26T07:11:37.183041Z"
    }
   },
   "cell_type": "code",
   "source": "one_liner_selector = Agent(name=\"Selector\", instructions=selection_agent_instructions + \"\\n\\n\\n Product context given to the advertising agent tools: \" + product, tools=tools, handoffs=[handoff_manager])",
   "id": "78c6c874ecda1de1",
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:11:49.759997Z",
     "start_time": "2026-01-26T07:11:39.281388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selector_message = \"Pick the best one-liner. Please mention the LLM model name used\"\n",
    "with trace(\"Selector verdict\"):\n",
    "    result = await Runner.run(one_liner_selector, selector_message)"
   ],
   "id": "95757c5be49aa903",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unleash primal power and eleganceBagheera, the ultimate feline companion for the bold and legendary. (Model: advertising_agent_tool_2)\n",
      "    ,       . (: advertising_agent_tool_2)\n"
     ]
    }
   ],
   "execution_count": 206
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
